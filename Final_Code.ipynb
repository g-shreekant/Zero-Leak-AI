{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1330d2-45c3-43e1-bda8-f1de06eb0e18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_13392\\1980971062.py:258: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7867\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import numpy as np\n",
    "import faiss\n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# ========= GLOBAL STATE =========\n",
    "# Global variables to store the extracted text, chunks, and model components.\n",
    "raw_text = \"\"\n",
    "chunks = []\n",
    "encoder = None\n",
    "index = None\n",
    "generator = None\n",
    "\n",
    "# ========= STEP 1: Process Uploaded PDF =========\n",
    "def process_pdf(file_path):\n",
    "    \"\"\"\n",
    "    Processes the uploaded PDF file: extracts text, chunks it, creates embeddings,\n",
    "    and initializes the language model.\n",
    "    \"\"\"\n",
    "    global raw_text, chunks, encoder, index, generator\n",
    "\n",
    "    # Check if a file was actually uploaded.\n",
    "    if file_path is None:\n",
    "        # If no file, return an error message and keep input/send disabled.\n",
    "        return \"⚠️ Please upload a PDF file.\", gr.update(value=\"\", interactive=False), gr.update(interactive=False)\n",
    "\n",
    "    try:\n",
    "        # 1. Extract text from the PDF using PyMuPDF (fitz).\n",
    "        doc = fitz.open(file_path)\n",
    "        raw_text = \"\".join(page.get_text() for page in doc)\n",
    "        doc.close() # Close the document after extraction\n",
    "\n",
    "        # 2. Chunk text using RecursiveCharacterTextSplitter for better context management.\n",
    "        # Chunks are 500 characters long with an overlap of 50 characters.\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "        chunks = splitter.split_text(raw_text)\n",
    "\n",
    "        # 3. Create embeddings for each chunk using SentenceTransformer.\n",
    "        # These embeddings are used for semantic search (RAG).\n",
    "        encoder = SentenceTransformer(\"./models/all-MiniLM-L6-v2\")\n",
    "        embeddings = encoder.encode(chunks)\n",
    "        dim = embeddings.shape[1] # Dimension of the embeddings\n",
    "\n",
    "        # Initialize FAISS index for efficient similarity search.\n",
    "        index = faiss.IndexFlatL2(dim) # L2 distance for similarity\n",
    "        index.add(np.array(embeddings)) # Add embeddings to the index\n",
    "\n",
    "        # 4. Load the flan-t5-large model for text generation.\n",
    "        # This model will answer questions based on the retrieved context.\n",
    "        flan_path = \"./models/flan-t5-large\"\n",
    "        embed_path = \"./models/all-MiniLM-L6-v2\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(flan_path)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(flan_path)\n",
    "        generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "        encoder = SentenceTransformer(embed_path)\n",
    "\n",
    "\n",
    "        # On successful processing, return a success message and enable the chat input/send button.\n",
    "        return \"✅ PDF uploaded and processed. You can now ask questions.\", gr.update(value=\"\", interactive=True), gr.update(interactive=True)\n",
    "    except Exception as e:\n",
    "        # If any error occurs during processing, return an error message and keep chat input/send disabled.\n",
    "        return f\"❌ Error processing PDF: {e}\", gr.update(value=\"\", interactive=False), gr.update(interactive=False)\n",
    "\n",
    "# ========= STEP 2: Handle User Questions =========\n",
    "\n",
    "def add_user_message(user_input, chat_history):\n",
    "    \"\"\"\n",
    "    Adds the user's message to the chat history immediately and clears the input box.\n",
    "    The bot's response will be added in a subsequent step.\n",
    "    \"\"\"\n",
    "    if not user_input.strip():\n",
    "        # If input is empty, do nothing and keep the current state.\n",
    "        return chat_history, gr.update(value=\"\", interactive=True)\n",
    "\n",
    "    # Append user message with None for the bot's response.\n",
    "    # Gradio will display a loading indicator for the bot's part.\n",
    "    chat_history.append((user_input, None))\n",
    "    # Return the updated chat history and clear/disable the input box\n",
    "    return chat_history, gr.update(value=\"\", interactive=False)\n",
    "\n",
    "def get_bot_response(chat_history):\n",
    "    \"\"\"\n",
    "    Generates the bot's response based on the last user message and updates the chat history.\n",
    "    This function is called after the user's message has been displayed.\n",
    "    \"\"\"\n",
    "    # Get the last user message from the chat history\n",
    "    user_input = chat_history[-1][0]\n",
    "\n",
    "    # Check if the model and PDF content are ready.\n",
    "    if not generator or not raw_text:\n",
    "        # If not ready, update the last chat entry with an error message.\n",
    "        chat_history[-1] = (user_input, \"⚠️ Please upload a PDF first.\")\n",
    "        # Return updated chat history and re-enable the input box.\n",
    "        return chat_history, gr.update(interactive=True)\n",
    "\n",
    "    try:\n",
    "        # Determine if the user wants a summary or a specific question answered.\n",
    "        if \"summarize\" in user_input.lower():\n",
    "            # For summarization, use the raw text (truncated to avoid exceeding model limits).\n",
    "            summary_text = raw_text[:4000]\n",
    "            prompt = f\"Summarize this document:\\n\\n{summary_text}\"\n",
    "        else:\n",
    "            # For question answering, retrieve relevant chunks using FAISS.\n",
    "            q_emb = encoder.encode([user_input]) # Embed the user's question\n",
    "            D, I = index.search(np.array(q_emb), 3) # Search for the top 3 most similar chunks\n",
    "            context = \"\\n\\n\".join([chunks[i] for i in I[0]]) # Combine retrieved chunks into context\n",
    "\n",
    "            # Construct the prompt with the retrieved context and the user's question.\n",
    "            max_context_length = 1500 # Approximate character limit for context\n",
    "            prompt = f\"Context:\\n{context[:max_context_length]}\\n\\nQuestion: {user_input}\"\n",
    "\n",
    "        # Generate the response using the loaded language model pipeline.\n",
    "        output = generator(prompt, max_new_tokens=200, do_sample=False, temperature=0.7)[0]['generated_text']\n",
    "        response = output.strip() # Clean up whitespace from the generated text\n",
    "\n",
    "        # Update the last entry in chat_history with the actual bot response.\n",
    "        chat_history[-1] = (user_input, response)\n",
    "    except Exception as e:\n",
    "        # If an error occurs during response generation, update the last entry with an error message.\n",
    "        chat_history[-1] = (user_input, f\"❌ Error generating response: {e}\")\n",
    "\n",
    "    # Return the updated chat history and re-enable the input box.\n",
    "    return chat_history, gr.update(interactive=True)\n",
    "\n",
    "# Function to clear chat and reset the application state.\n",
    "def clear_all():\n",
    "    \"\"\"Resets all global state variables and clears the UI.\"\"\"\n",
    "    global raw_text, chunks, encoder, index, generator\n",
    "    raw_text = \"\"\n",
    "    chunks = []\n",
    "    encoder = None\n",
    "    index = None\n",
    "    generator = None\n",
    "    # Return empty chat history, reset status, and disable input/send button.\n",
    "    return [], \"Upload a PDF to begin...\", gr.update(interactive=False), gr.update(interactive=False)\n",
    "\n",
    "# ========= STEP 3: Launch Gradio Interface =========\n",
    "# Define the Gradio Blocks interface with custom theming and CSS for enhanced UI.\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(), # Apply a soft, modern theme\n",
    "    css=\"\"\"\n",
    "    /* General body styling */\n",
    "    body {\n",
    "        font-family: 'Inter', sans-serif; /* Use Inter font */\n",
    "        background-color: #f0f2f5; /* Light grey background */\n",
    "    }\n",
    "    /* Main container styling */\n",
    "    .gradio-container {\n",
    "        max-width: 1200px; /* Increased max width for the app */\n",
    "        margin: auto; /* Center the container */\n",
    "        padding: 20px;\n",
    "        box-shadow: 0 4px 12px rgba(0,0,0,0.05); /* Subtle shadow for depth */\n",
    "        border-radius: 12px; /* Rounded corners */\n",
    "        background-color: #ffffff; /* White background for the app */\n",
    "    }\n",
    "    /* Title styling */\n",
    "    h2 {\n",
    "        text-align: center;\n",
    "        color: #333; /* Dark grey text */\n",
    "        margin-bottom: 20px;\n",
    "        font-size: 1.8em; /* Larger font size */\n",
    "        font-weight: 500; /* Bold font */\n",
    "    }\n",
    "    /* Button styling */\n",
    "    .gr-button {\n",
    "        border-radius: 8px !important; /* Rounded buttons */\n",
    "        padding: 10px 20px !important;\n",
    "        font-weight: 600 !important;\n",
    "    }\n",
    "    /* File upload component styling */\n",
    "    #file-upload-component {\n",
    "        border-radius: 8px !important;\n",
    "        border: 1px solid #e0e0e0 !important;\n",
    "    }\n",
    "    /* Textbox styling */\n",
    "    .gr-textbox {\n",
    "        border-radius: 8px !important;\n",
    "        border: 1px solid #e0e0e0 !important;\n",
    "    }\n",
    "    /* Chatbot component styling */\n",
    "    #main-chatbot {\n",
    "        border-radius: 8px !important;\n",
    "        border: 1px solid #e0e0e0 !important;\n",
    "        min-height: 400px; /* Ensure a decent height for the chat area */\n",
    "        overflow-y: auto; /* Enable scrolling for chat history */\n",
    "        background-color: #fdfdfd; /* Slightly off-white background for chat */\n",
    "    }\n",
    "    /* Bot message bubble styling */\n",
    "    .message.bot {\n",
    "        background-color: #e6f7ff; /* Light blue for bot messages */\n",
    "        border-radius: 10px;\n",
    "        padding: 10px;\n",
    "        margin: 5px 0;\n",
    "        word-break: break-word; /* Ensure long words break and wrap */\n",
    "    }\n",
    "    /* User message bubble styling */\n",
    "    .message.user {\n",
    "        background-color: #f0f0f0; /* Light gray for user messages */\n",
    "        border-radius: 10px;\n",
    "        padding: 10px;\n",
    "        margin: 5px 0;\n",
    "        word-break: break-word; /* Ensure long words break and wrap */\n",
    "    }\n",
    "    /* Message wrapper padding */\n",
    "    .message-wrap {\n",
    "        padding: 5px 0;\n",
    "    }\n",
    "    /* Column and Row spacing */\n",
    "    .gr-column, .gr-row {\n",
    "        gap: 20px; /* Space between columns/rows */\n",
    "    }\n",
    "    /* Specific styling for the status box */\n",
    "    #upload-status-box {\n",
    "        background-color: #e9ecef; /* Slightly darker background for status */\n",
    "        color: #495057;\n",
    "        font-weight: 500;\n",
    "    }\n",
    "    /* Styling for the send button */\n",
    "    #send-button {\n",
    "        background-color: #007bff; /* Primary blue color */\n",
    "        color: white;\n",
    "    }\n",
    "    #send-button:hover {\n",
    "        background-color: #0056b3; /* Darker blue on hover */\n",
    "    }\n",
    "    /* Styling for the clear button */\n",
    "    #clear-button {\n",
    "        background-color: #dc3545; /* Red color for clear */\n",
    "        color: white;\n",
    "    }\n",
    "    #clear-button:hover {\n",
    "        background-color: #c82333; /* Darker red on hover */\n",
    "    }\n",
    "    /* Styling for the message input box */\n",
    "    #message-input {\n",
    "        border: 1px solid #007bff !important; /* Add a blue border */\n",
    "        border-radius: 8px !important; /* Keep rounded corners */\n",
    "        padding: 10px !important; /* Add some padding */\n",
    "    }\n",
    "\"\"\") as demo:\n",
    "    # Main title and subtitle\n",
    "    gr.Markdown(\"## 🧠ZeroLeak AI - Your confidential documents, your questions — all offline\\nUpload your PDF and ask anything - 100% private\", elem_classes=[\"title\"])\n",
    "\n",
    "    with gr.Row(): # Use a row to divide the layout into two main columns\n",
    "        with gr.Column(scale=1): # Left column for controls (smaller)\n",
    "            # File upload component\n",
    "            file_upload = gr.File(label=\"📄 Upload PDF\", type=\"filepath\", file_types=[\".pdf\"], elem_id=\"file-upload-component\")\n",
    "            # Status textbox to show processing messages\n",
    "            upload_status = gr.Textbox(label=\"Status\", interactive=False, placeholder=\"Upload a PDF to begin...\", elem_id=\"upload-status-box\")\n",
    "            # Button to clear chat and reset the application\n",
    "            clear_button = gr.Button(\"Clear Chat & Reset\", elem_id=\"clear-button\")\n",
    "\n",
    "        with gr.Column(scale=3): # Right column for the chatbot (larger)\n",
    "            # Chatbot component to display conversation history\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Chat History\",\n",
    "                height=500, # Fixed height for consistent layout\n",
    "                show_copy_button=True, # Allow copying messages\n",
    "                avatar_images=(None, \"robotic_11804593.png\"), # User avatar (None) and example bot avatar\n",
    "                elem_id=\"main-chatbot\"\n",
    "            )\n",
    "            # Textbox for user input, initially disabled\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Ask a question or type 'summarize'\",\n",
    "                placeholder=\"e.g. What are the main findings? Or type 'summarize'\",\n",
    "                scale=7, # Takes more space in the row\n",
    "                container=False, # Remove default container for cleaner look\n",
    "                interactive=False, # Initially disabled until PDF is processed\n",
    "                elem_id=\"message-input\"\n",
    "            )\n",
    "            # Send button for user input, initially disabled\n",
    "            send_button = gr.Button(\"Send\", scale=1, interactive=False, elem_id=\"send-button\") # Takes less space, next to msg\n",
    "\n",
    "    # Event Handlers:\n",
    "\n",
    "    # When a file is uploaded, call process_pdf.\n",
    "    # Outputs update the status, and enable/disable the message input and send button.\n",
    "    file_upload.change(\n",
    "        fn=process_pdf,\n",
    "        inputs=file_upload,\n",
    "        outputs=[upload_status, msg, send_button],\n",
    "        show_progress=True # Show progress indicator during upload/processing\n",
    "    )\n",
    "\n",
    "    # When the clear button is clicked, reset the app state.\n",
    "    clear_button.click(\n",
    "        fn=clear_all,\n",
    "        inputs=[],\n",
    "        outputs=[chatbot, upload_status, msg, send_button]\n",
    "    )\n",
    "\n",
    "    # Chain the message submission:\n",
    "    # 1. Add user message to chat and clear input.\n",
    "    # 2. Then, get bot response and update chat.\n",
    "    msg.submit(\n",
    "        fn=add_user_message,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, msg],\n",
    "        show_progress=False # No progress for immediate user message display\n",
    "    ).then(\n",
    "        fn=get_bot_response,\n",
    "        inputs=[chatbot],\n",
    "        outputs=[chatbot, msg],\n",
    "        show_progress=True # Show progress for bot generation\n",
    "    )\n",
    "\n",
    "    send_button.click(\n",
    "        fn=add_user_message,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[chatbot, msg],\n",
    "        show_progress=False # No progress for immediate user message display\n",
    "    ).then(\n",
    "        fn=get_bot_response,\n",
    "        inputs=[chatbot],\n",
    "        outputs=[chatbot, msg],\n",
    "        show_progress=True # Show progress for bot generation\n",
    "    )\n",
    "\n",
    "demo.launch(show_api=False)\n",
    "\n",
    "def add_user_message_final(user_input, chat_history):\n",
    "    \"\"\"\n",
    "    Adds the user's message to the chat history immediately and clears the input box.\n",
    "    The bot's response will be added in a subsequent step.\n",
    "    \"\"\"\n",
    "    if not user_input.strip():\n",
    "        # If input is empty, do nothing and keep the current state.\n",
    "        return chat_history, gr.update(value=\"\", interactive=True)\n",
    "\n",
    "    # Append user message with None for the bot's response.\n",
    "    # Gradio will display a loading indicator for the bot's part.\n",
    "    chat_history.append((user_input, None))\n",
    "    # Return the updated chat history and clear/disable the input box\n",
    "    return chat_history, gr.update(value=\"\", interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4fed51-d975-4ee2-866e-3f1ee4daced9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
